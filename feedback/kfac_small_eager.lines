Timer unit: 1e-06 s

Total time: 10.9289 s
File: kfac_small_eager_test.py
Function: loss_and_grad at line 195

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   195                                           @profile
   196                                           def loss_and_grad(Wf):
   197                                             """Returns cost, gradient for current parameter vector."""
   198                                             
   199        10       685389  68538.9      6.3    W = unflatten(Wf, fs[1:])   # perftodo: this creates transposes
   200        10           36      3.6      0.0    W.insert(0, X)
   201                                           
   202        10           25      2.5      0.0    A = [None]*(n+2)
   203        10           21      2.1      0.0    A[1] = W[0]
   204        30           92      3.1      0.0    for i in range(1, n+1):
   205        20       531758  26587.9      4.9      A[i+1] = tf.sigmoid(W[i] @ A[i])
   206        10       281988  28198.8      2.6    err = (A[3] - A[1])
   207                                           
   208        10           32      3.2      0.0    def d_sigmoid(y):
   209                                               return y*(1-y)
   210                                           
   211        10           32      3.2      0.0    B = [None]*(n+1)
   212        10           19      1.9      0.0    B2 = [None]*(n+1)
   213        10        94160   9416.0      0.9    B[n] = err*d_sigmoid(A[n+1])
   214        10       353552  35355.2      3.2    sampled_labels = tf.random_normal((f(n), f(-1)), dtype=dtype, seed=0)
   215        10       119087  11908.7      1.1    B2[n] = sampled_labels*d_sigmoid(A[n+1])
   216        30          245      8.2      0.0    for i in range(n-1, -1, -1):
   217        20       210488  10524.4      1.9      backprop = t(W[i+1]) @ B[i+1]
   218        20       210956  10547.8      1.9      backprop2 = t(W[i+1]) @ B2[i+1]
   219        20       672573  33628.7      6.2      B[i] = backprop*d_sigmoid(A[i+1])
   220        20       929803  46490.2      8.5      B2[i] = backprop2*d_sigmoid(A[i+1])
   221                                           
   222        10           31      3.1      0.0    dW = [None]*(n+1)
   223        10           20      2.0      0.0    pre_dW = [None]*(n+1)  # preconditioned dW
   224                                           
   225        10           17      1.7      0.0    cov_A = [None]*(n+1)    # covariance of activations[i]
   226        10           14      1.4      0.0    cov_B2 = [None]*(n+1)   # covariance of synthetic backprops[i]
   227        10           17      1.7      0.0    vars_svd_A = [None]*(n+1)
   228        10           19      1.9      0.0    vars_svd_B2 = [None]*(n+1)
   229        30          103      3.4      0.0    for i in range(1,n+1):
   230        20      2115165 105758.2     19.4      cov_A[i] = A[i]@t(A[i])/dsize
   231        20       658925  32946.2      6.0      cov_B2[i] = B2[i]@t(B2[i])/dsize
   232        20      1330192  66509.6     12.2      whitened_A = regularized_inverse(cov_A[i], lambda_) @ A[i]
   233        20       779507  38975.3      7.1      whitened_B = regularized_inverse(cov_B2[i], lambda_) @ B[i]
   234        20       445826  22291.3      4.1      pre_dW[i] = (whitened_B @ t(whitened_A))/dsize
   235        20      1394772  69738.6     12.8      dW[i] = (B[i] @ t(A[i]))/dsize
   236                                           
   237        10        52161   5216.1      0.5    reconstruction = L2(err) / (2 * dsize)
   238        10           27      2.7      0.0    loss = reconstruction
   239                                           
   240        10        29138   2913.8      0.3    grad = flatten(dW[1:])
   241        10        32715   3271.5      0.3    kfac_grad = flatten(pre_dW[1:])
   242        10           23      2.3      0.0    return loss, grad, kfac_grad

