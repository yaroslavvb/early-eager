Timer unit: 1e-06 s

Total time: 5.47739 s
File: kfac_small_final_test.py
Function: main at line 74

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    74                                           @profile
    75                                           def main():
    76         1           24     24.0      0.0    np.random.seed(0)
    77         1           81     81.0      0.0    tf.set_random_seed(0)
    78                                             
    79         1           15     15.0      0.0    dtype = np.float32
    80                                             # 64-bit doesn't help much, search for 64-bit in
    81                                             # https://www.wolframcloud.com/objects/5f297f41-30f7-4b1b-972c-cac8d1f8d8e4
    82         1           16     16.0      0.0    u.default_dtype = dtype
    83         1           27     27.0      0.0    machine_epsilon = np.finfo(dtype).eps # 1e-7 or 1e-16
    84         1       172367 172367.0      3.1    train_images = load_MNIST.load_MNIST_images('data/train-images-idx3-ubyte')
    85         1           20     20.0      0.0    dsize = 10000
    86         1           31     31.0      0.0    patches = train_images[:,:dsize];
    87         1           15     15.0      0.0    fs = [dsize, 28*28, 196, 28*28]
    88                                           
    89                                             # values from deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial
    90         1           14     14.0      0.0    X0=patches
    91         1           14     14.0      0.0    lambda_=3e-3
    92         1          957    957.0      0.0    rho=tf.constant(0.1, dtype=dtype)
    93         1           15     15.0      0.0    beta=3
    94         1         4042   4042.0      0.1    W0f = W_uniform(fs[2],fs[3])
    95                                           
    96         1            8      8.0      0.0    def f(i): return fs[i+1]  # W[i] has shape f[i] x f[i-1]
    97         1           15     15.0      0.0    dsize = f(-1)
    98         1           16     16.0      0.0    n = len(fs) - 2
    99                                           
   100                                             # helper to create variables with numpy or TF initial value
   101         1           15     15.0      0.0    init_dict = {}     # {var_placeholder: init_value}
   102         1            7      7.0      0.0    vard = {}          # {var: util.VarInfo}
   103         1           14     14.0      0.0    def init_var(val, name, trainable=False, noinit=False):
   104                                               if isinstance(val, tf.Tensor):
   105                                                 collections = [] if noinit else None
   106                                                 var = tf.Variable(val, name=name, collections=collections)
   107                                               else:
   108                                                 val = np.array(val)
   109                                                 assert u.is_numeric, "Unknown type"
   110                                                 holder = tf.placeholder(dtype, shape=val.shape, name=name+"_holder")
   111                                                 var = tf.Variable(holder, name=name, trainable=trainable)
   112                                                 init_dict[holder] = val
   113                                               var_p = tf.placeholder(var.dtype, var.shape)
   114                                               var_setter = var.assign(var_p)
   115                                               vard[var] = u.VarInfo(var_setter, var_p)
   116                                               return var
   117                                           
   118         1         5981   5981.0      0.1    lr = init_var(0.2, "lr")
   119         1           16     16.0      0.0    if purely_linear:   # need lower LR without sigmoids
   120                                               lr = init_var(.02, "lr")
   121                                               
   122         1         6456   6456.0      0.1    Wf = init_var(W0f, "Wf", True)
   123         1         8197   8197.0      0.1    Wf_copy = init_var(W0f, "Wf_copy", True)
   124         1        55939  55939.0      1.0    W = u.unflatten(Wf, fs[1:])   # perftodo: this creates transposes
   125         1        19504  19504.0      0.4    X = init_var(X0, "X")
   126         1            9      9.0      0.0    W.insert(0, X)
   127                                           
   128         1            8      8.0      0.0    def sigmoid(x):
   129                                               if not purely_linear:
   130                                                 return tf.sigmoid(x)
   131                                               else:
   132                                                 return tf.identity(x)
   133                                                 
   134         1            6      6.0      0.0    def d_sigmoid(y):
   135                                               if not purely_linear:
   136                                                 return y*(1-y)
   137                                               else:
   138                                                 return 1
   139                                               
   140         1            6      6.0      0.0    def kl(x, y):
   141                                               return x * tf.log(x / y) + (1 - x) * tf.log((1 - x) / (1 - y))
   142         1           10     10.0      0.0    def d_kl(x, y):
   143                                               return (1-x)/(1-y) - x/y
   144                                             
   145                                             # A[i] = activations needed to compute gradient of W[i]
   146                                             # A[n+1] = network output
   147         1            8      8.0      0.0    A = [None]*(n+2)
   148                                           
   149                                             # A[0] is just for shape checks, assert fail on run
   150                                             # tf.assert always fails because of static assert
   151                                             # fail_node = tf.assert_equal(1, 0, message="too huge")
   152         1         5476   5476.0      0.1    fail_node = tf.Print(0, [0], "fail, this must never run")
   153         1           68     68.0      0.0    with tf.control_dependencies([fail_node]):
   154         1         3524   3524.0      0.1      A[0] = u.Identity(dsize, dtype=dtype)
   155         1           23     23.0      0.0    A[1] = W[0]
   156         3           54     18.0      0.0    for i in range(1, n+1):
   157         2         6370   3185.0      0.1      A[i+1] = sigmoid(W[i] @ A[i])
   158                                               
   159                                             # reconstruction error and sparsity error
   160         1         1007   1007.0      0.0    err = (A[3] - A[1])
   161         1         4251   4251.0      0.1    rho_hat = tf.reduce_sum(A[2], axis=1, keep_dims=True)/dsize
   162                                           
   163                                             # B[i] = backprops needed to compute gradient of W[i]
   164                                             # B2[i] = backprops from sampled labels needed for natural gradient
   165         1           15     15.0      0.0    B = [None]*(n+1)
   166         1            7      7.0      0.0    B2 = [None]*(n+1)
   167         1         3303   3303.0      0.1    B[n] = err*d_sigmoid(A[n+1])
   168         1         4836   4836.0      0.1    sampled_labels_live = tf.random_normal((f(n), f(-1)), dtype=dtype, seed=0)
   169         1         4883   4883.0      0.1    sampled_labels = init_var(sampled_labels_live, "sampled_labels", noinit=True)
   170         1         3365   3365.0      0.1    B2[n] = sampled_labels*d_sigmoid(A[n+1])
   171         3           40     13.3      0.0    for i in range(n-1, -1, -1):
   172         2        15424   7712.0      0.3      backprop = t(W[i+1]) @ B[i+1]
   173         2        15397   7698.5      0.3      backprop2 = t(W[i+1]) @ B2[i+1]
   174         2           24     12.0      0.0      if i == 1 and not drop_sparsity:
   175                                                 backprop += beta*d_kl(rho, rho_hat)
   176                                                 backprop2 += beta*d_kl(rho, rho_hat)
   177         2         6536   3268.0      0.1      B[i] = backprop*d_sigmoid(A[i+1])
   178         2         6446   3223.0      0.1      B2[i] = backprop2*d_sigmoid(A[i+1])
   179                                           
   180                                             # dW[i] = gradient of W[i]
   181         1           16     16.0      0.0    dW = [None]*(n+1)
   182         1           14     14.0      0.0    pre_dW = [None]*(n+1)  # preconditioned dW
   183         1           14     14.0      0.0    pre_dW_stable = [None]*(n+1)  # preconditioned stable dW
   184                                           
   185         1           14     14.0      0.0    cov_A = [None]*(n+1)    # covariance of activations[i]
   186         1           14     14.0      0.0    cov_B2 = [None]*(n+1)   # covariance of synthetic backprops[i]
   187         1            6      6.0      0.0    vars_svd_A = [None]*(n+1)
   188         1            7      7.0      0.0    vars_svd_B2 = [None]*(n+1)
   189         3           48     16.0      0.0    for i in range(1,n+1):
   190         2        27065  13532.5      0.5      cov_A[i] = init_var(A[i]@t(A[i])/dsize, "cov_A%d"%(i,))
   191         2        27703  13851.5      0.5      cov_B2[i] = init_var(B2[i]@t(B2[i])/dsize, "cov_B2%d"%(i,))
   192         2        67235  33617.5      1.2      vars_svd_A[i] = u.SvdWrapper(cov_A[i],"svd_A_%d"%(i,))
   193         2        64067  32033.5      1.2      vars_svd_B2[i] = u.SvdWrapper(cov_B2[i],"svd_B2_%d"%(i,))
   194         2           22     11.0      0.0      if use_tikhonov:
   195         2        46672  23336.0      0.9        whitened_A = u.regularized_inverse2(vars_svd_A[i],L=Lambda) @ A[i]
   196                                               else:
   197                                                 whitened_A = u.pseudo_inverse2(vars_svd_A[i]) @ A[i]
   198         2           30     15.0      0.0      if use_tikhonov:
   199         2        57822  28911.0      1.1        whitened_B2 = u.regularized_inverse2(vars_svd_B2[i],L=Lambda) @ B[i]
   200                                               else:
   201                                                 whitened_B2 = u.pseudo_inverse2(vars_svd_B2[i]) @ B[i]
   202         2        29828  14914.0      0.5      whitened_A_stable = u.pseudo_inverse_sqrt2(vars_svd_A[i]) @ A[i]
   203         2        30071  15035.5      0.5      whitened_B2_stable = u.pseudo_inverse_sqrt2(vars_svd_B2[i]) @ B[i]
   204         2        17738   8869.0      0.3      pre_dW[i] = (whitened_B2 @ t(whitened_A))/dsize
   205         2        18395   9197.5      0.3      pre_dW_stable[i] = (whitened_B2_stable @ t(whitened_A_stable))/dsize
   206         2        17843   8921.5      0.3      dW[i] = (B[i] @ t(A[i]))/dsize
   207                                           
   208                                             # Loss function
   209         1         3779   3779.0      0.1    reconstruction = u.L2(err) / (2 * dsize)
   210         1        12812  12812.0      0.2    sparsity = beta * tf.reduce_sum(kl(rho, rho_hat))
   211         1         7011   7011.0      0.1    L2 = (lambda_ / 2) * (u.L2(W[1]) + u.L2(W[1]))
   212                                           
   213         1           14     14.0      0.0    loss = reconstruction
   214         1           14     14.0      0.0    if not drop_l2:
   215                                               loss = loss + L2
   216         1           14     14.0      0.0    if not drop_sparsity:
   217                                               loss = loss + sparsity
   218                                           
   219         1        21537  21537.0      0.4    grad_live = u.flatten(dW[1:])
   220         1        21642  21642.0      0.4    pre_grad_live = u.flatten(pre_dW[1:]) # fisher preconditioned gradient
   221         1        21740  21740.0      0.4    pre_grad_stable_live = u.flatten(pre_dW_stable[1:]) # sqrt fisher preconditioned grad
   222         1         4565   4565.0      0.1    grad = init_var(grad_live, "grad")
   223         1         4510   4510.0      0.1    pre_grad = init_var(pre_grad_live, "pre_grad")
   224         1         4538   4538.0      0.1    pre_grad_stable = init_var(pre_grad_stable_live, "pre_grad_stable")
   225                                           
   226         1         2822   2822.0      0.1    update_params_op = Wf.assign(Wf-lr*pre_grad).op
   227         1         2841   2841.0      0.1    update_params_stable_op = Wf.assign(Wf-lr*pre_grad_stable).op
   228         1         1137   1137.0      0.0    save_params_op = Wf_copy.assign(Wf).op
   229         1         2676   2676.0      0.0    pre_grad_dot_grad = tf.reduce_sum(pre_grad*grad)
   230         1         2651   2651.0      0.0    pre_grad_stable_dot_grad = tf.reduce_sum(pre_grad*grad)
   231         1         2630   2630.0      0.0    grad_norm = tf.reduce_sum(grad*grad)
   232         1         2488   2488.0      0.0    pre_grad_norm = u.L2(pre_grad)
   233         1         2533   2533.0      0.0    pre_grad_stable_norm = u.L2(pre_grad_stable)
   234                                           
   235         1           17     17.0      0.0    def dump_svd_info(step):
   236                                               """Dump singular values and gradient values in those coordinates."""
   237                                               for i in range(1, n+1):
   238                                                 svd = vars_svd_A[i]
   239                                                 s0, u0, v0 = sess.run([svd.s, svd.u, svd.v])
   240                                                 util.dump(s0, "A_%d_%d"%(i, step))
   241                                                 A0 = A[i].eval()
   242                                                 At0 = v0.T @ A0
   243                                                 util.dump(A0 @ A0.T, "Acov_%d_%d"%(i, step))
   244                                                 util.dump(At0 @ At0.T, "Atcov_%d_%d"%(i, step))
   245                                                 util.dump(s0, "As_%d_%d"%(i, step))
   246                                           
   247                                               for i in range(1, n+1):
   248                                                 svd = vars_svd_B2[i]
   249                                                 s0, u0, v0 = sess.run([svd.s, svd.u, svd.v])
   250                                                 util.dump(s0, "B2_%d_%d"%(i, step))
   251                                                 B0 = B[i].eval()
   252                                                 Bt0 = v0.T @ B0
   253                                                 util.dump(B0 @ B0.T, "Bcov_%d_%d"%(i, step))
   254                                                 util.dump(Bt0 @ Bt0.T, "Btcov_%d_%d"%(i, step))
   255                                                 util.dump(s0, "Bs_%d_%d"%(i, step))      
   256                                               
   257         1           14     14.0      0.0    def advance_batch():
   258                                               sess.run(sampled_labels.initializer)  # new labels for next call
   259                                           
   260         1           16     16.0      0.0    def update_covariances():
   261                                               ops_A = [cov_A[i].initializer for i in range(1, n+1)]
   262                                               ops_B2 = [cov_B2[i].initializer for i in range(1, n+1)]
   263                                               sess.run(ops_A+ops_B2)
   264                                           
   265         1           14     14.0      0.0    def update_svds():
   266                                               if whitening_mode>1:
   267                                                 vars_svd_A[2].update()
   268                                               if whitening_mode>2:
   269                                                 vars_svd_B2[2].update()
   270                                               if whitening_mode>3:
   271                                                 vars_svd_B2[1].update()
   272                                           
   273         1           14     14.0      0.0    def init_svds():
   274                                               """Initialize our SVD to identity matrices."""
   275                                               ops = []
   276                                               for i in range(1, n+1):
   277                                                 ops.extend(vars_svd_A[i].init_ops)
   278                                                 ops.extend(vars_svd_B2[i].init_ops)
   279                                               sess = tf.get_default_session()
   280                                               sess.run(ops)
   281                                                 
   282         1         1138   1138.0      0.0    init_op = tf.global_variables_initializer()
   283                                             #  tf.get_default_graph().finalize()
   284                                             
   285         1       898501 898501.0     16.4    sess = tf.InteractiveSession()
   286         1       284238 284238.0      5.2    sess.run(Wf.initializer, feed_dict=init_dict)
   287         1        56160  56160.0      1.0    sess.run(X.initializer, feed_dict=init_dict)
   288         1        10736  10736.0      0.2    advance_batch()
   289         1       170996 170996.0      3.1    update_covariances()
   290         1        90059  90059.0      1.6    init_svds()
   291         1       190834 190834.0      3.5    sess.run(init_op, feed_dict=init_dict)  # initialize everything else
   292                                             
   293         1           52     52.0      0.0    print("Running training.")
   294         1           20     20.0      0.0    u.reset_time()
   295                                           
   296         1           16     16.0      0.0    step_lengths = []     # keep track of learning rates
   297         1            7      7.0      0.0    losses = []
   298         1           16     16.0      0.0    ratios = []           # actual loss decrease / expected decrease
   299         1            7      7.0      0.0    grad_norms = []       
   300         1            7      7.0      0.0    pre_grad_norms = []   # preconditioned grad norm squared
   301         1            7      7.0      0.0    pre_grad_stable_norms = [] # sqrt preconditioned grad norms squared
   302         1            7      7.0      0.0    target_delta_list = []     # predicted decrease linear approximation
   303         1            7      7.0      0.0    target_delta2_list = []    # predicted decrease quadratic appromation
   304         1            7      7.0      0.0    actual_delta_list = []      # actual decrease
   305                                             
   306                                             # adaptive line search parameters
   307         1           16     16.0      0.0    alpha=0.3   # acceptable fraction of predicted decrease
   308         1           16     16.0      0.0    beta=0.8    # how much to shrink when violation
   309         1           16     16.0      0.0    growth_rate=1.05  # how much to grow when too conservative
   310                                               
   311         1           17     17.0      0.0    def update_cov_A(i):
   312                                               sess.run(cov_A[i].initializer)
   313         1           16     16.0      0.0    def update_cov_B2(i):
   314                                               sess.run(cov_B2[i].initializer)
   315                                           
   316                                             # only update whitening matrix of input activations in the beginning
   317         1           16     16.0      0.0    if whitening_mode>0:
   318         1       118589 118589.0      2.2      vars_svd_A[1].update()
   319                                           
   320                                             # compute t(delta).H.delta/2
   321         1           21     21.0      0.0    def hessian_quadratic(delta):
   322                                               #    update_covariances()
   323                                               W = u.unflatten(delta, fs[1:])
   324                                               W.insert(0, None)
   325                                               total = 0
   326                                               for l in range(1, n+1):
   327                                                 decrement = tf.trace(t(W[l])@cov_B2[l]@W[l]@cov_A[l])
   328                                                 total+=decrement
   329                                               return (total/2).eval()
   330                                               
   331                                             # compute t(delta).H^-1.delta/2
   332         1           19     19.0      0.0    def hessian_quadratic_inv(delta):
   333                                               #    update_covariances()
   334                                               W = u.unflatten(delta, fs[1:])
   335                                               W.insert(0, None)
   336                                               total = 0
   337                                               for l in range(1, n+1):
   338                                                 invB2 = u.pseudo_inverse2(vars_svd_B2[l])
   339                                                 invA = u.pseudo_inverse2(vars_svd_A[l])
   340                                                 decrement = tf.trace(t(W[l])@invB2@W[l]@invA)
   341                                                 total+=decrement
   342                                               return (total/2).eval()
   343                                           
   344                                             # do line search, dump values as csv
   345         1           18     18.0      0.0    def line_search(initial_value, direction, step, num_steps):
   346                                               saved_val = tf.Variable(Wf)
   347                                               sess.run(saved_val.initializer)
   348                                               pl = tf.placeholder(dtype, shape=(), name="linesearch_p")
   349                                               assign_op = Wf.assign(initial_value - direction*step*pl)
   350                                               vals = []
   351                                               for i in range(num_steps):
   352                                                 sess.run(assign_op, feed_dict={pl: i})
   353                                                 vals.append(loss.eval())
   354                                               sess.run(Wf.assign(saved_val)) # restore original value
   355                                               return vals
   356                                               
   357        21          321     15.3      0.0    for step in range(num_steps): 
   358        20       207724  10386.2      3.8      update_covariances()
   359        20          379     18.9      0.0      if step % whiten_every_n_steps==0:
   360        20      1782503  89125.1     32.5        update_svds()
   361                                           
   362        20       145532   7276.6      2.7      sess.run(grad.initializer)
   363        20       361172  18058.6      6.6      sess.run(pre_grad.initializer)
   364                                               
   365        20        70473   3523.7      1.3      lr0, loss0 = sess.run([lr, loss])
   366        20        12650    632.5      0.2      save_params_op.run()
   367                                           
   368                                               # regular inverse becomes unstable when grad norm exceeds 1
   369        20        15477    773.9      0.3      stabilized_mode = grad_norm.eval()<1
   370                                           
   371        20          353     17.6      0.0      if stabilized_mode and not use_tikhonov:
   372                                                 update_params_stable_op.run()
   373                                               else:
   374        20        13176    658.8      0.2        update_params_op.run()
   375                                           
   376        20        66286   3314.3      1.2      loss1 = loss.eval()
   377        20        26307   1315.3      0.5      advance_batch()
   378                                           
   379                                               # line search stuff
   380        20        10943    547.1      0.2      target_slope = (-pre_grad_dot_grad.eval() if stabilized_mode else
   381         9        10073   1119.2      0.2                      -pre_grad_stable_dot_grad.eval())
   382        20          366     18.3      0.0      target_delta = lr0*target_slope
   383        20          348     17.4      0.0      target_delta_list.append(target_delta)
   384                                           
   385                                               # second order prediction of target delta
   386                                               # TODO: the sign is wrong, debug this
   387                                               # https://www.wolframcloud.com/objects/8f287f2f-ceb7-42f7-a599-1c03fda18f28
   388        20          303     15.2      0.0      if local_quadratics:
   389                                                 x0 = Wf_copy.eval()
   390                                                 x_opt = x0-pre_grad.eval()
   391                                                 # computes t(x)@H^-1 @(x)/2
   392                                                 y_opt = loss0 - hessian_quadratic_inv(grad)
   393                                                 # computes t(x)@H @(x)/2
   394                                                 y_expected = hessian_quadratic(Wf-x_opt)+y_opt
   395                                                 target_delta2 = y_expected - loss0
   396                                                 target_delta2_list.append(target_delta2)
   397                                                 
   398                                               
   399        20          327     16.4      0.0      actual_delta = loss1 - loss0
   400        20          349     17.4      0.0      actual_slope = actual_delta/lr0
   401        20          343     17.1      0.0      slope_ratio = actual_slope/target_slope  # between 0 and 1.01
   402        20          347     17.4      0.0      actual_delta_list.append(actual_delta)
   403                                           
   404        20          325     16.2      0.0      if do_line_search:
   405                                                 vals1 = line_search(Wf_copy, pre_grad, lr/100, 40)
   406                                                 vals2 = line_search(Wf_copy, grad, lr/100, 40)
   407                                                 u.dump(vals1, "line1-%d"%(i,))
   408                                                 u.dump(vals2, "line2-%d"%(i,))
   409                                                 
   410        20          346     17.3      0.0      losses.append(loss0)
   411        20          332     16.6      0.0      step_lengths.append(lr0)
   412        20          328     16.4      0.0      ratios.append(slope_ratio)
   413        20         8632    431.6      0.2      grad_norms.append(grad_norm.eval())
   414        20        14747    737.4      0.3      pre_grad_norms.append(pre_grad_norm.eval())
   415        20        14591    729.5      0.3      pre_grad_stable_norms.append(pre_grad_stable_norm.eval())
   416                                           
   417        20          373     18.6      0.0      if step % report_frequency == 0:
   418         7         5907    843.9      0.1        print("Step %d loss %.2f, target decrease %.3f, actual decrease, %.3f ratio %.2f grad norm: %.2f pregrad norm: %.2f"%(step, loss0, target_delta, actual_delta, slope_ratio, grad_norm.eval(), pre_grad_norm.eval()))
   419                                               
   420        20          344     17.2      0.0      if adaptive_step_frequency and adaptive_step and step>adaptive_step_burn_in:
   421                                                 # shrink if wrong prediction, don't shrink if prediction is tiny
   422                                                 if slope_ratio < alpha and abs(target_delta)>1e-6 and adaptive_step:
   423                                                   print("%.2f %.2f %.2f"%(loss0, loss1, slope_ratio))
   424                                                   print("Slope optimality %.2f, shrinking learning rate to %.2f"%(slope_ratio, lr0*beta,))
   425                                                   sess.run(vard[lr].setter, feed_dict={vard[lr].p: lr0*beta})
   426                                                   
   427                                                 # grow learning rate, slope_ratio .99 worked best for gradient
   428                                                 elif step>0 and i%50 == 0 and slope_ratio>0.90 and adaptive_step:
   429                                                     print("%.2f %.2f %.2f"%(loss0, loss1, slope_ratio))
   430                                                     print("Growing learning rate to %.2f"%(lr0*growth_rate))
   431                                                     sess.run(vard[lr].setter, feed_dict={vard[lr].p:
   432                                                                                          lr0*growth_rate})
   433                                           
   434        20          598     29.9      0.0      u.record_time()
   435                                           
   436                                             # check against expected loss
   437         1           11     11.0      0.0    if 'Apple' in sys.version:
   438                                               pass
   439                                               #    u.dump(losses, "kfac_small_final_mac.csv")
   440                                               targets = np.loadtxt("data/kfac_small_final_mac.csv", delimiter=",")
   441                                             else:
   442                                               pass
   443                                               #    u.dump(losses, "kfac_small_final_linux.csv")
   444         1         1269   1269.0      0.0      targets = np.loadtxt("data/kfac_small_final_linux.csv", delimiter=",")
   445                                           
   446         1          376    376.0      0.0    u.check_equal(targets, losses[:len(targets)], rtol=1e-1)
   447         1          320    320.0      0.0    u.summarize_time()
   448         1           21     21.0      0.0    print("Test passed")

